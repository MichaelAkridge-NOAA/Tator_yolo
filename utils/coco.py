"""COCO dataset helpers."""

from __future__ import annotations

import json
import logging
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import base64
import numpy as np
try:  # optional dependency
    from scipy.spatial import ConvexHull
except Exception:  # pragma: no cover - optional
    ConvexHull = None

logger = logging.getLogger(__name__)


def _coco_has_invalid_image_refs_impl(path: Path) -> bool:
    try:
        with path.open("r", encoding="utf-8") as handle:
            data = json.load(handle)
    except Exception:
        return True
    if not isinstance(data, dict):
        return True
    images = data.get("images")
    anns = data.get("annotations")
    if not isinstance(images, list) or not isinstance(anns, list):
        return True
    image_ids = set()
    for img in images:
        if not isinstance(img, dict):
            continue
        try:
            image_ids.add(int(img.get("id")))
        except Exception:
            continue
    for ann in anns:
        if not isinstance(ann, dict):
            continue
        try:
            img_id = int(ann.get("image_id"))
        except Exception:
            continue
        if img_id not in image_ids:
            return True
    return False


def _coco_missing_segmentation_impl(path: Path) -> bool:
    try:
        with path.open("r", encoding="utf-8") as handle:
            data = json.load(handle)
    except Exception:
        return True
    if not isinstance(data, dict):
        return True
    anns = data.get("annotations")
    if not isinstance(anns, list):
        return True
    for ann in anns:
        if not isinstance(ann, dict):
            continue
        seg = ann.get("segmentation")
        if seg is not None and seg != []:
            return False
    return True


def _coco_info_block_impl(dataset_id: str) -> Dict[str, Any]:
    """Minimal COCO info section to keep pycocotools happy."""
    return {
        "description": f"{dataset_id} generated by tator",
        "version": "1.0",
        "year": int(time.strftime("%Y", time.gmtime())),
        "contributor": "tator",
        "date_created": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
    }


def _write_coco_annotations_impl(
    output_path: Path,
    *,
    dataset_id: str,
    categories: List[Dict[str, Any]],
    images: List[Dict[str, Any]],
    annotations: List[Dict[str, Any]],
) -> None:
    payload = {
        "info": _coco_info_block_impl(dataset_id),
        "licenses": [],
        "images": images,
        "annotations": annotations,
        "categories": categories,
    }
    with output_path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle)


def _ensure_coco_info_fields_impl(path: Path, dataset_id: str, categories: List[Dict[str, Any]]) -> str:
    """Backfill missing COCO 'info'/'licenses' for older conversions."""
    try:
        with path.open("r", encoding="utf-8") as handle:
            data = json.load(handle)
    except Exception as exc:  # noqa: BLE001
        logger.warning("Failed to load COCO file %s to backfill info: %s", path, exc)
        return str(path)
    if not isinstance(data, dict):
        return str(path)
    modified = False
    if "info" not in data or not isinstance(data["info"], dict):
        data["info"] = _coco_info_block_impl(dataset_id)
        modified = True
    if "licenses" not in data or not isinstance(data["licenses"], list):
        data["licenses"] = []
        modified = True
    if categories and (not isinstance(data.get("categories"), list) or not data["categories"]):
        data["categories"] = categories
        modified = True
    if modified:
        try:
            with path.open("w", encoding="utf-8") as handle:
                json.dump(data, handle)
        except Exception as exc:  # noqa: BLE001
            logger.warning("Failed to rewrite COCO file %s: %s", path, exc)
    return str(path)


def _ensure_coco_supercategory_impl(path: Path, default: str = "object") -> bool:
    """Ensure every COCO category has a supercategory (RF-DETR expects it)."""
    try:
        with path.open("r", encoding="utf-8") as handle:
            data = json.load(handle)
    except Exception:
        return False
    if not isinstance(data, dict):
        return False
    categories = data.get("categories")
    if not isinstance(categories, list):
        return False
    modified = False
    for category in categories:
        if not isinstance(category, dict):
            continue
        if "supercategory" not in category:
            category["supercategory"] = default
            modified = True
    if modified:
        try:
            with path.open("w", encoding="utf-8") as handle:
                json.dump(data, handle)
        except Exception:
            return False
    return modified


def _encode_binary_mask_impl(mask: np.ndarray, *, max_bytes: int = 0) -> Optional[Dict[str, Any]]:
    try:
        mask_arr = np.asarray(mask)
    except Exception:
        return None
    if mask_arr.ndim == 3 and mask_arr.shape[-1] == 1:
        mask_arr = mask_arr[..., 0]
    if mask_arr.ndim != 2:
        return None
    mask_bool = mask_arr.astype(bool)
    height, width = mask_bool.shape
    packed = np.packbits(mask_bool.astype(np.uint8), axis=None)
    try:
        packed_bytes = packed.tobytes()
    except Exception:
        return None
    if max_bytes > 0 and len(packed_bytes) > max_bytes:
        return None
    try:
        encoded = base64.b64encode(packed_bytes).decode("ascii")
    except Exception:
        return None
    return {"size": [int(height), int(width)], "counts": encoded}


def _decode_binary_mask_impl(payload: Dict[str, Any]) -> Optional[np.ndarray]:
    if not payload:
        return None
    counts = payload.get("counts")
    size = payload.get("size") or []
    if not counts or len(size) != 2:
        return None
    try:
        packed = np.frombuffer(base64.b64decode(counts), dtype=np.uint8)
        bits = np.unpackbits(packed)[: int(size[0]) * int(size[1])]
        return bits.reshape(int(size[0]), int(size[1]))
    except Exception:
        return None


def _rdp(points: np.ndarray, epsilon: float) -> np.ndarray:
    """Ramer–Douglas–Peucker simplification for 2D points."""
    if points.shape[0] < 3 or epsilon <= 0:
        return points

    def _perp_dist(pt, start, end):
        if np.allclose(start, end):
            return np.linalg.norm(pt - start)
        return np.abs(np.cross(end - start, start - pt)) / np.linalg.norm(end - start)

    start_pt = points[0]
    end_pt = points[-1]
    dmax = 0.0
    idx = 0
    for i in range(1, len(points) - 1):
        d = _perp_dist(points[i], start_pt, end_pt)
        if d > dmax:
            idx = i
            dmax = d
    if dmax > epsilon:
        rec1 = _rdp(points[: idx + 1], epsilon)
        rec2 = _rdp(points[idx:], epsilon)
        return np.concatenate((rec1[:-1], rec2), axis=0)
    return np.array([start_pt, end_pt])


def _mask_to_polygon_impl(mask: np.ndarray, simplify_epsilon: float) -> List[Tuple[float, float]]:
    """Extract a coarse polygon outline from a binary mask."""
    try:
        mask_arr = np.asarray(mask).astype(bool)
    except Exception:
        return []
    if mask_arr.ndim != 2 or not mask_arr.any():
        return []
    coords = np.argwhere(mask_arr)  # y, x
    if coords.shape[0] < 3:
        return []
    points = np.stack([coords[:, 1], coords[:, 0]], axis=1)  # x, y
    hull_pts = points
    if ConvexHull is not None:
        try:
            hull = ConvexHull(points)
            hull_pts = points[hull.vertices]
        except Exception:
            hull_pts = points
    if simplify_epsilon and simplify_epsilon > 0:
        hull_pts = _rdp(hull_pts, simplify_epsilon)
    # Ensure at least 3 points.
    if hull_pts.shape[0] < 3:
        xs, ys = points[:, 0], points[:, 1]
        x1, x2 = xs.min(), xs.max()
        y1, y2 = ys.min(), ys.max()
        hull_pts = np.array([[x1, y1], [x2, y1], [x2, y2], [x1, y2]])
    return [(float(x), float(y)) for x, y in hull_pts]
